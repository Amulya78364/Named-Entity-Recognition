# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DKyXh8-yt5xVtRWjh7JzVj1HbgT2Yif1
"""

import tensorflow as tf
tf.config.list_physical_devices('GPU')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import random
import csv
import matplotlib.pyplot as plt
import re
import string
from collections import defaultdict
import torch
from torch import nn, optim
from torch.optim import Adam
from torch.autograd import Variable
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences #install
from transformers import BertTokenizer, BertForTokenClassification, RobertaTokenizer, RobertaForTokenClassification, AdamW
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, precision_score, recall_score, precision_recall_fscore_support
from joblib import dump, load
import time
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

sns.set_style('darkgrid')
from keras.models import Sequential
from keras.layers import Input, Embedding,TimeDistributed, LSTM, Dropout, Bidirectional, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from keras.utils import plot_model, to_categorical
from sklearn.model_selection import train_test_split

dataset_file = "/content/drive/MyDrive/BTC/train.txt"
train_file = "/content/drive/MyDrive/BTC1/train.txt"
dev_file = "/content/drive/MyDrive/BTC1/validation.txt"
test_file = "/content/drive/MyDrive/BTC1/test.txt"

def load_words(data_file, delimiter='\t'):
    words = []
    tags = []
    reader = open(data_file).readlines()
    for i, row in enumerate(reader):
        row = row.strip()  # Remove leading and trailing whitespace
        if delimiter == '\t':
            row = row.split('\t')
        else:
            row = row.split()
        if len(row) == 1:
            words.append(row[0])
            tags.append('')
        elif len(row) == 2:
            words.append(row[0])
            tags.append(row[1].strip())  # Remove leading and trailing whitespace from the tag
    return words, tags

train_words, train_tags = load_words(train_file)
dev_words, dev_tags = load_words(dev_file)
test_words, test_tags = load_words(test_file)

words, tags = load_words(dataset_file)

print("Train words:", train_words)
print("Train tags:", train_tags)

print("Dev words:", dev_words[:20])
print("Dev tags:", dev_tags[:20])

print("Test words:", test_words[:20])
print("Test tags:", test_tags[:50])

import pandas as pd

# Create DataFrame
train_df = pd.DataFrame({'words': train_words, 'tags': train_tags})

dev_df = pd.DataFrame({'words': dev_words, 'tags': dev_tags})

# Define custom style for alternating row colors
def style_df(df):
    return df.style.set_table_styles([{'selector': 'tr:nth-of-type(odd)', 'props': [('background-color', '#f9f9f9')]}])

# Apply custom style and display the DataFrame
styled_train_df = style_df(dev_df[27:60])
styled_train_df

unique_tags = ['B-LOC','B-ORG','B-PER','I-LOC','I-ORG','I-PER','O']
tag_index = dict((tag, i) for i, tag in enumerate(unique_tags))
index_tag = dict((i, tag) for i, tag in enumerate(unique_tags))

print(tag_index)

# Make sentences
def get_sentences(words):
    sentence = []
    sentence_list = []
    for word in words:
        if word != '':
          # print("word:", word)
          sentence.append(word)
        else:
          # print(word)
          sentence_list.append(sentence)
          # print(sentence_list)
          sentence = []
    print(sentence_list[0])
    return sentence_list

train_sentences = get_sentences(train_words)
train_sent_tags = get_sentences(train_tags)
dev_sentences = get_sentences(dev_words)
dev_sent_tags = get_sentences(dev_tags)
test_sentences = get_sentences(test_words)
test_sent_tags = get_sentences(test_tags)
print("dev sentences", dev_sentences)
print("test sent tags", test_sent_tags)

sentences = get_sentences(words)
sent_tags = get_sentences(tags)
print("sentences", sentences)
print("sent tags", sent_tags)

# print("dev sent:", max(len(dev_sentences)))
print("dev sent:", max([len(sentence) for sentence in dev_sentences]))
print("dev sent tags", len(dev_sent_tags) )


## Tokenize
def tokenize_sentences(sentences, sentence_tags, tokenizer):
    #sentences = train_sentences
    #sentence_tags = train_sent_tags

    token_sentences = []
    label_sentences = []
    origin_sentences = []

    for j in range(0,len(sentences)):
        word_list = sentences[j]
        tag_list = sentence_tags[j]
        tokens = []
        tags = []
        origins = []

        for i, word in enumerate(word_list):
            # print('sentence',j,'word',i)
            token = tokenizer.tokenize(word) #tokenize word according to BERT
            tokens.extend(token)
            tag = tag_list[i]
            # fit labels to tokenized size of word
            for m in range(len(token)):
                tags.append(tag)
                if m == 0:
                  origins.append(True)
                else:
                  origins.append(False)
        token_sentences.append(tokens)
        label_sentences.append(tags)
        origin_sentences.append(origins)

    return token_sentences, label_sentences, origin_sentences, tokenizer

fig, ax = plt.subplots(figsize=(20, 6))
ax.hist([len(s) for s in sentences], bins=50)
ax.set_title('Number of words in each Sentence')

maxlen = max([len(s) for s in sentences])
print('Number of Sentences:', len(sentences))
print ('Maximum sequence length:', maxlen)

bilstm_words = list(set(words))
bilstm_words.remove('')
bilstm_words.append("ENDPAD")
n_words = len(bilstm_words)
print('Number of unique words:', n_words)

tags1 = pd.Series(tags)
tags1 = tags1[tags1 != '']
fig, ax = plt.subplots(2, 1, figsize=(20, 12))
tags1.value_counts().plot.bar(ax=ax[0], title='Distribution of Tags')
tags1[tags1 != 'O'].value_counts().plot.bar(ax=ax[1], title='Distribution of non-O Tags')

tags = list(set(train_tags))
tags.remove('')
n_tags = len(tags)
print('Number of unique Tags:', n_tags)

# Tokenize using BERT tokenizer
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
train_word_tokens_bert, train_tag_tokens_bert, train_origin_bert, _ = tokenize_sentences(train_sentences, train_sent_tags, bert_tokenizer)
dev_word_tokens_bert, dev_tag_tokens_bert, dev_origin_bert, _ = tokenize_sentences(dev_sentences, dev_sent_tags, bert_tokenizer)
test_word_tokens_bert, test_tag_tokens_bert, test_origin_bert, _ = tokenize_sentences(test_sentences, test_sent_tags, bert_tokenizer)

# Tokenize using RoBERTa tokenizer
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
train_word_tokens_roberta, train_tag_tokens_roberta, train_origin_roberta, _ = tokenize_sentences(train_sentences, train_sent_tags, roberta_tokenizer)
dev_word_tokens_roberta, dev_tag_tokens_roberta, dev_origin_roberta, _ = tokenize_sentences(dev_sentences, dev_sent_tags, roberta_tokenizer)
test_word_tokens_roberta, test_tag_tokens_roberta, test_origin_roberta, _ = tokenize_sentences(test_sentences, test_sent_tags, roberta_tokenizer)

MAX_LENGTH = 105 #max sentence length

def token2id(token_sentences, label_sentences, tag_index, tokenizer):

    token_ids = [tokenizer.convert_tokens_to_ids(sentence) for sentence in token_sentences]
    token_ids = pad_sequences(token_ids, maxlen=MAX_LENGTH, dtype="long", truncating="post", padding="post")

    if label_sentences != '':
        label_ids = [[tag_index[label] for label in sentence] for sentence in label_sentences]
        label_ids = pad_sequences(label_ids, maxlen=MAX_LENGTH, value=tag_index["O"], padding="post", dtype="long", truncating="post")
    else:
        label_ids = ''
    #attention mask to ignore padding
    attention_masks = [[float(word_id>0) for word_id in sentence] for sentence in token_ids]

    return token_ids, label_ids, attention_masks, tokenizer

# Convert tokens to IDs for BERT
train_word_ids_bert, train_tag_ids_bert, train_attention_bert, _ = token2id(train_word_tokens_bert, train_tag_tokens_bert, tag_index, bert_tokenizer)
dev_word_ids_bert, dev_tag_ids_bert, dev_attention_bert, _ = token2id(dev_word_tokens_bert, dev_tag_tokens_bert, tag_index, bert_tokenizer)
test_word_ids_bert, test_tag_ids_bert, test_attention_bert, _ = token2id(test_word_tokens_bert, test_tag_tokens_bert, tag_index, bert_tokenizer)

# Convert tokens to IDs for RoBERTa
train_word_ids_roberta, train_tag_ids_roberta, train_attention_roberta, _ = token2id(train_word_tokens_roberta, train_tag_tokens_roberta, tag_index, roberta_tokenizer)
dev_word_ids_roberta, dev_tag_ids_roberta, dev_attention_roberta, _ = token2id(dev_word_tokens_roberta, dev_tag_tokens_roberta, tag_index, roberta_tokenizer)
test_word_ids_roberta, test_tag_ids_roberta, test_attention_roberta, _ = token2id(test_word_tokens_roberta, test_tag_tokens_roberta, tag_index, roberta_tokenizer)

# Define hyperparameters
LEARNING_RATE = 2e-5
BATCH_SIZE = 32
NUM_EPOCHS = 5

# Load BERT tokenizer and model
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
bert_model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(tag_index))

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bert_model.to(device)

# Define optimizer and scheduler
bert_optimizer = AdamW(bert_model.parameters(), lr=LEARNING_RATE)


# Define training and evaluation functions
def train_epoch(model, train_loader, optimizer):
    model.train()
    total_loss = 0
    all_labels = []
    all_preds = []
    for batch in train_loader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        preds = torch.argmax(outputs.logits, dim=-1)
        all_labels.extend(labels.cpu().numpy().flatten())
        all_preds.extend(preds.cpu().numpy().flatten())
        loss.backward()
        optimizer.step()
    accuracy = accuracy_score(all_labels, all_preds)
    return total_loss / len(train_loader), accuracy

def evaluate(model, data_loader):
    model.eval()
    all_labels = []
    all_preds = []
    total_loss = 0
    with torch.no_grad():
        for batch in data_loader:
            input_ids, attention_mask, labels = batch
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
            preds = torch.argmax(outputs.logits, dim=-1)
            all_labels.extend(labels.cpu().numpy().flatten())
            all_preds.extend(preds.cpu().numpy().flatten())
    accuracy = accuracy_score(all_labels, all_preds)
    avg_loss = total_loss / len(data_loader)
    return accuracy, avg_loss

# Prepare data loaders
train_data_bert = TensorDataset(torch.tensor(train_word_ids_bert), torch.tensor(train_attention_bert), torch.tensor(train_tag_ids_bert))
train_sampler_bert = RandomSampler(train_data_bert)
train_loader_bert = DataLoader(train_data_bert, sampler=train_sampler_bert, batch_size=BATCH_SIZE)

dev_data_bert = TensorDataset(torch.tensor(dev_word_ids_bert), torch.tensor(dev_attention_bert), torch.tensor(dev_tag_ids_bert))
dev_loader_bert = DataLoader(dev_data_bert, batch_size=BATCH_SIZE)

# Training loop
train_losses = []
dev_losses = []
train_accuracies = []
dev_accuracies = []

for epoch in range(NUM_EPOCHS):
    train_loss_bert, train_accuracy_bert = train_epoch(bert_model, train_loader_bert, bert_optimizer)
    dev_accuracy_bert, dev_loss_bert = evaluate(bert_model, dev_loader_bert)

    # Store metrics
    train_losses.append(train_loss_bert)
    dev_losses.append(dev_loss_bert)
    train_accuracies.append(train_accuracy_bert)
    dev_accuracies.append(dev_accuracy_bert)

    print(f'Epoch {epoch + 1}: Train Loss of BERT = {train_loss_bert:.4f}, Train Accuracy of BERT = {train_accuracy_bert:.4f}, Val Loss of BERT= {dev_loss_bert:.4f}, Val Accuracy of BERT = {dev_accuracy_bert:.4f}')

# Plotting
epochs = range(1, NUM_EPOCHS + 1)

plt.figure(figsize=(14, 5))

# Plotting train and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses, label='Train Loss')
plt.plot(epochs, dev_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

# Plotting train and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracies, label='Train Accuracy')
plt.plot(epochs, dev_accuracies, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test set
test_data_bert = TensorDataset(torch.tensor(test_word_ids_bert), torch.tensor(test_attention_bert), torch.tensor(test_tag_ids_bert))
test_loader_bert = DataLoader(test_data_bert, batch_size=BATCH_SIZE)
test_accuracy_bert, test_loss_bert = evaluate(bert_model, test_loader_bert)
print(f'Test Accuracy using BERT = {test_accuracy_bert:.4f}')

# Get predictions and true labels
all_preds_bert = []
all_labels_bert = []
with torch.no_grad():
    for batch in test_loader_bert:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
        outputs = bert_model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1)
        all_preds_bert.extend(preds.cpu().numpy().flatten())
        all_labels_bert.extend(labels.cpu().numpy().flatten())

# Map index to tag
index_tag = {i: tag for tag, i in tag_index.items()}
predicted_tags_bert = [index_tag[pred] for pred in all_preds_bert]
true_tags_bert = [index_tag[label] for label in all_labels_bert]

# Calculate tag-wise accuracy
bert_accuracy = classification_report(true_tags_bert, predicted_tags_bert, target_names=unique_tags, output_dict=True)

print("\nClassification Report for BERT Model:\n")
print(classification_report(true_tags_bert, predicted_tags_bert))

precision, recall, f1_score, _ = precision_recall_fscore_support(true_tags_bert, predicted_tags_bert, average='weighted')

# Print overall precision, recall, and F1-score
print("Overall Precision:", precision)
print("Overall Recall:", recall)
print("Overall F1-score:", f1_score)

# Print tag-wise accuracy
print("\nTag-wise Accuracy:")
for tag, metrics in bert_accuracy.items():
    if tag in tag_index:
        accuracy = metrics['f1-score']
        print(f"{tag}: {accuracy:.4f}")

# Calculate confusion matrix
conf_matrix = confusion_matrix(true_tags_bert, predicted_tags_bert)

# Plot confusion matrix with different colormap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=unique_tags, yticklabels=unique_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('Actual Labels')
plt.title('Confusion Matrix')
plt.show()

# Normalize confusion matrix
conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1, keepdims=True)

# Plot normalized confusion matrix with different colormap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_norm, annot=True, cmap='Reds', xticklabels=unique_tags, yticklabels=unique_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Normalized Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Extract precision, recall, and F1-score for each tag
bert_precision = [bert_accuracy[tag]['precision'] for tag in unique_tags]
bert_recall = [bert_accuracy[tag]['recall'] for tag in unique_tags]
bert_f1_score = [bert_accuracy[tag]['f1-score'] for tag in unique_tags]

# Create a bar plot
plt.figure(figsize=(12, 6))
bar_width = 0.3
index = range(len(unique_tags))

plt.bar(index, bert_precision, color='b', width=bar_width, label='Precision')
plt.bar([i + bar_width for i in index], bert_recall, color='g', width=bar_width, label='Recall')
plt.bar([i + 2 * bar_width for i in index], bert_f1_score, color='r', width=bar_width, label='F1-score')

plt.xlabel('NER Tags')
plt.ylabel('Scores')
plt.title('Class-wise Metrics for Named Entity Recognition using BERT')
plt.xticks([i + bar_width for i in index], unique_tags, rotation=90)
plt.legend()
plt.tight_layout()
plt.show()

# Load RoBERTa tokenizer and model
roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta_model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(tag_index))

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
roberta_model.to(device)

# Define optimizer and scheduler
roberta_optimizer = AdamW(roberta_model.parameters(), lr=LEARNING_RATE)

# Prepare data loaders
train_data_roberta = TensorDataset(torch.tensor(train_word_ids_roberta), torch.tensor(train_attention_roberta), torch.tensor(train_tag_ids_roberta))
train_sampler_roberta = RandomSampler(train_data_roberta)
train_loader_roberta = DataLoader(train_data_roberta, sampler=train_sampler_roberta, batch_size=BATCH_SIZE)

dev_data_roberta = TensorDataset(torch.tensor(dev_word_ids_roberta), torch.tensor(dev_attention_roberta), torch.tensor(dev_tag_ids_roberta))
dev_loader_roberta = DataLoader(dev_data_roberta, batch_size=BATCH_SIZE)

# Training loop
train_losses_roberta = []
dev_losses_roberta = []
train_accuracies_roberta = []
dev_accuracies_roberta = []

for epoch in range(NUM_EPOCHS):
    train_loss_roberta, train_accuracy_roberta = train_epoch(roberta_model, train_loader_roberta, roberta_optimizer)
    dev_accuracy_roberta, dev_loss_roberta = evaluate(roberta_model, dev_loader_roberta)

     # Store metrics
    train_losses_roberta.append(train_loss_roberta)
    dev_losses_roberta.append(dev_loss_roberta)
    train_accuracies_roberta.append(train_accuracy_roberta)
    dev_accuracies_roberta.append(dev_accuracy_roberta)

    # print(f'Epoch {epoch + 1}: Train Loss of RoBERTa = {train_loss_roberta:.4f}, Val Accuracy of RoBERTa = {dev_accuracy_roberta:.4f}')
    print(f'Epoch {epoch + 1}: Train Loss of RoBERTa = {train_loss_roberta:.4f}, Train Accuracy of RoBERTa = {train_accuracy_roberta:.4f}, Val Loss of RoBERTa = {dev_loss_roberta:.4f}, Val Accuracy of RoBERTa= {dev_accuracy_roberta:.4f}')

# Plotting
epochs = range(1, NUM_EPOCHS + 1)

plt.figure(figsize=(14, 5))

# Plotting train and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses_roberta, label='Train Loss')
plt.plot(epochs, dev_losses_roberta, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

# Plotting train and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracies_roberta, label='Train Accuracy')
plt.plot(epochs, dev_accuracies_roberta, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test set
test_data_roberta = TensorDataset(torch.tensor(test_word_ids_roberta), torch.tensor(test_attention_roberta), torch.tensor(test_tag_ids_roberta))
test_loader_roberta = DataLoader(test_data_roberta, batch_size=BATCH_SIZE)
test_accuracy_roberta, test_loss_roberta = evaluate(roberta_model, test_loader_roberta)
print(f'Test Accuracy using RoBERTa = {test_accuracy_roberta:.4f}')

all_preds_roberta = []
all_labels_roberta = []
with torch.no_grad():
    for batch in test_loader_roberta:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
        outputs = roberta_model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1)
        all_preds_roberta.extend(preds.cpu().numpy().flatten())
        all_labels_roberta.extend(labels.cpu().numpy().flatten())

# Map index to tag
index_tag = {i: tag for tag, i in tag_index.items()}
predicted_tags_roberta = [index_tag[pred] for pred in all_preds_roberta]
true_tags_roberta = [index_tag[label] for label in all_labels_roberta]

# Calculate tag-wise accuracy
roberta_accuracy = classification_report(true_tags_roberta, predicted_tags_roberta, target_names=unique_tags, output_dict=True)

print("\nClassification Report for RoBERTa Model:\n")
print(classification_report(true_tags_roberta, predicted_tags_roberta))

precision, recall, f1_score, _ = precision_recall_fscore_support(true_tags_bert, predicted_tags_bert, average='weighted')

# Print overall precision, recall, and F1-score
print("Overall Precision:", precision)
print("Overall Recall:", recall)
print("Overall F1-score:", f1_score)


# Print tag-wise accuracy
print("\nTag-wise Accuracy:")
for tag, metrics in roberta_accuracy.items():
    if tag in tag_index:
        accuracy = metrics['f1-score']
        print(f"{tag}: {accuracy:.4f}")

# Calculate confusion matrix
conf_matrix = confusion_matrix(true_tags_roberta, predicted_tags_roberta)

# Plot confusion matrix with different colormap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=unique_tags, yticklabels=unique_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('Actual Labels')
plt.title('Confusion Matrix')
plt.show()

# Normalize confusion matrix
conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1, keepdims=True)

# Plot normalized confusion matrix with different colormap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_norm, annot=True, cmap='Reds', xticklabels=unique_tags, yticklabels=unique_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Normalized Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Extract precision, recall, and F1-score for each tag
roberta_precision = [roberta_accuracy[tag]['precision'] for tag in unique_tags]
roberta_recall = [roberta_accuracy[tag]['recall'] for tag in unique_tags]
roberta_f1_score = [roberta_accuracy[tag]['f1-score'] for tag in unique_tags]

# Create a bar plot
plt.figure(figsize=(12, 6))
bar_width = 0.3
index = range(len(unique_tags))

plt.bar(index, roberta_precision, color='b', width=bar_width, label='Precision')
plt.bar([i + bar_width for i in index], roberta_recall, color='g', width=bar_width, label='Recall')
plt.bar([i + 2 * bar_width for i in index], roberta_f1_score, color='r', width=bar_width, label='F1-score')

plt.xlabel('NER Tags')
plt.ylabel('Scores')
plt.title('Class-wise Metrics for Named Entity Recognition using RoBERTa')
plt.xticks([i + bar_width for i in index], unique_tags, rotation=90)
plt.legend()
plt.tight_layout()
plt.show()

def get_sentences(words):
    sentence = []
    sentence_list = []
    for word in words:
        if word != '':
            sentence.append(word)
        else:
            sentence_list.append(sentence)
            sentence = []
    return sentence_list

train_sentences = get_sentences(train_words)
train_labels = get_sentences(train_tags)
validation_sentences = get_sentences(dev_words)
validation_labels = get_sentences(dev_tags)
test_sentences = get_sentences(test_words)
test_labels = get_sentences(test_tags)

# Create vocabulary and label set from all data
word_set = set([word for sentence in train_sentences + validation_sentences + test_sentences for word in sentence])
label_set = set([label for label_list in train_labels + validation_labels + test_labels for label in label_list])

print("Word set:", word_set)
print("Label set:", label_set)

print("Length of Word set:", len(word_set))
print("Length of Label set:", len(label_set))

word2idx = {word: idx + 1 for idx, word in enumerate(word_set)}
label2idx = {label: idx for idx, label in enumerate(label_set)}

# Convert words and labels to indices
X_train = [[word2idx[word] for word in sentence] for sentence in train_sentences]
X_val = [[word2idx.get(word, 0) for word in sentence] for sentence in validation_sentences]
X_test = [[word2idx.get(word, 0) for word in sentence] for sentence in test_sentences]

print(X_train[0])
print(X_val[1])
print(X_test[0])

y_train = [[label2idx[label] for label in label_list] for label_list in train_labels]
y_val = [[label2idx[label] for label in label_list] for label_list in validation_labels]
y_test = [[label2idx[label] for label in label_list] for label_list in test_labels]

# Padding sequences
max_len = max(max(len(sent) for sent in X_train),
              max(len(sent) for sent in X_val),
              max(len(sent) for sent in X_test))
print(max_len)
X_train = pad_sequences(X_train, maxlen=max_len, padding='post')
X_val = pad_sequences(X_val, maxlen=max_len, padding='post')
X_test = pad_sequences(X_test, maxlen=max_len, padding='post')

y_train = pad_sequences(y_train, maxlen=max_len, padding='post', value=label2idx['O'])
y_val = pad_sequences(y_val, maxlen=max_len, padding='post', value=label2idx['O'])
y_test = pad_sequences(y_test, maxlen=max_len, padding='post', value=label2idx['O'])

# Define the model
vocab_size = len(word_set) + 1
embedding_dim = 100
num_labels = len(label_set)
hidden_dim = 100

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))
model.add(LSTM(hidden_dim, return_sequences=True))
model.add(Dense(num_labels, activation='softmax'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=7, batch_size=32)

import matplotlib.pyplot as plt

# Access training history
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Create a figure and specify the subplot layout
plt.figure(figsize=(12, 5))

# Plot training and validation loss
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(train_accuracy, label='Train Accuracy')
plt.plot(val_accuracy, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test data
loss, accuracy = model.evaluate(X_test, y_test)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

# Predict on test data
probabilities = model.predict(X_test)
predictions = np.argmax(probabilities, axis=-1)

# Convert indices back to labels
idx2label = {idx: label for label, idx in label2idx.items()}
pred_tags = [[idx2label[idx] for idx in row] for row in predictions]
ground_truth_tags = [[idx2label[idx] for idx in row] for row in y_test]

# Print ground truth and predicted tags
for i in range(len(test_sentences)):
    print("Sentence:", ' '.join(test_sentences[i]))
    print("Ground Truth Tags:", ' '.join(ground_truth_tags[i]))
    print("Predicted Tags:", ' '.join(pred_tags[i]))
    print()

from sklearn.metrics import classification_report

# Flatten the ground truth and predicted tags
y_true_flat = [tag for tag_list in ground_truth_tags for tag in tag_list]
pred_tags_flat = [tag for tag_list in pred_tags for tag in tag_list]

# Print classification report
print(classification_report(y_true_flat, pred_tags_flat))

from sklearn.metrics import precision_recall_fscore_support
# Calculate precision, recall, and F1-score for each class
precision, recall, f1_score, _ = precision_recall_fscore_support(y_true_flat, pred_tags_flat, average='weighted')

# Print overall precision, recall, and F1-score
print("Overall Precision:", precision)
print("Overall Recall:", recall)
print("Overall F1-score:", f1_score)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Flatten the ground truth and predicted tags
y_true_flat = [label for label_list in y_test for label in label_list]
pred_tags_flat = [idx for row in predictions for idx in row]

# Get unique labels and sort them in ascending order
unique_labels = sorted(set(y_true_flat + pred_tags_flat))

# Compute confusion matrix
conf_matrix = confusion_matrix(y_true_flat, pred_tags_flat, labels=unique_labels)

# Sort the confusion matrix and labels based on the sum of each row (accuracy)
sorted_indices = np.argsort(np.sum(conf_matrix, axis=1))
conf_matrix_sorted = conf_matrix[sorted_indices][:, sorted_indices]
sorted_labels = [idx2label[unique_labels[i]] for i in sorted_indices]

print(sorted_labels)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_sorted, annot=True, fmt='d', cmap='Reds', xticklabels=sorted_labels, yticklabels=sorted_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Compute normalized confusion matrix
cm_normalized = confusion_matrix(y_true_flat, pred_tags_flat, labels=unique_labels, normalize='true')
cm_normalized_sorted = cm_normalized[sorted_indices][:, sorted_indices]

# Plot normalized confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized_sorted, annot=True, cmap='Reds', xticklabels=sorted_labels, yticklabels=sorted_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Normalized Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

# Generate classification report
classification_rep = classification_report(y_true_flat, pred_tags_flat, labels=unique_labels, target_names=label_set, output_dict=True)

print(sorted_labels)

# # Get the tag names from sorted_labels using idx2label dictionary
# sorted_tag_names = [idx2label[label_idx] for label_idx in sorted_labels]

# # Print the sorted tag names
# print(sorted_tag_names)


# Extract precision, recall, and F1-score for each tag
precision = [classification_rep[tag]['precision'] for tag in label_set]
recall = [classification_rep[tag]['recall'] for tag in label_set]
f1_score = [classification_rep[tag]['f1-score'] for tag in label_set]

# Sort tags and corresponding metrics
sorted_tags = [tag for _, tag in sorted(zip(label_set, label_set))]
sorted_precision = [prec for _, prec in sorted(zip(label_set, precision))]
sorted_recall = [rec for _, rec in sorted(zip(label_set, recall))]
sorted_f1_score = [f1 for _, f1 in sorted(zip(label_set, f1_score))]

print(sorted_labels)

# Create a bar plot
plt.figure(figsize=(12, 6))
bar_width = 0.3
index = range(len(sorted_labels))

plt.bar(index, sorted_precision, color='b', width=bar_width, label='Precision')
plt.bar([i + bar_width for i in index], sorted_recall, color='g', width=bar_width, label='Recall')
plt.bar([i + 2 * bar_width for i in index], sorted_f1_score, color='r', width=bar_width, label='F1-score')

plt.xlabel('NER Tags')
plt.ylabel('Scores')
plt.title('Class-wise Metrics for Named Entity Recognition using LSTM')
plt.xticks([i + bar_width for i in index], sorted_tags, rotation=90)
plt.legend()
plt.tight_layout()
plt.show()

def get_sentences(words):
    sentence = []
    sentence_list = []
    for word in words:
        if word != '':
            sentence.append(word)
        else:
            sentence_list.append(sentence)
            sentence = []
    return sentence_list

train_sentences = get_sentences(train_words)
train_labels = get_sentences(train_tags)
validation_sentences = get_sentences(dev_words)
validation_labels = get_sentences(dev_tags)
test_sentences = get_sentences(test_words)
test_labels = get_sentences(test_tags)

# Define vocabulary and tags
words = list(set(train_words + dev_words + test_words))
words.remove('')
words.append("ENDPAD")
n_words = len(words)
tags = list(set(train_tags + dev_tags + test_tags))
tags.remove('')
n_tags = len(tags)

maxlen = max([len(s) for s in train_sentences + validation_sentences + test_sentences])
print(maxlen)

# Convert words and tags to indices
word2idx = {w: i for i, w in enumerate(words)}
tag2idx = {t: i for i, t in enumerate(tags)}

# Convert sentences and labels to sequences of indices
def words_to_indices(sentences, labels):
    X = [[word2idx[w] for w in s] for s in sentences]
    X = sequence.pad_sequences(maxlen=maxlen, sequences=X, padding="post", value=n_words - 1)
    y = [[tag2idx[w] for w in s] for s in labels]
    y = sequence.pad_sequences(maxlen=maxlen, sequences=y, padding="post", value=tag2idx["O"])
    y = np.array([to_categorical(i, num_classes=n_tags) for i in y])
    return X, y

# Convert train, validation, and test data to sequences of indices
X_train, y_train = words_to_indices(train_sentences, train_labels)
X_val, y_val = words_to_indices(validation_sentences, validation_labels)
X_test, y_test = words_to_indices(test_sentences, test_labels)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

class config():
    VOCAB = n_words
    MAX_LEN = maxlen
    N_OUPUT = n_tags

    EMBEDDING_VECTOR_LENGTH = 50
    N_LSTM_CELLS = 64
    RECURRENT_DROPOUT = 0.1

    OUTPUT_ACTIVATION = 'softmax'

    LOSS = 'categorical_crossentropy'
    OPTIMIZER = 'adam'
    METRICS = ['accuracy']

    MAX_EPOCHS = 5

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)
rlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)


model = Sequential()
model.add(Embedding(input_dim=config.VOCAB, output_dim=config.EMBEDDING_VECTOR_LENGTH, input_length=config.MAX_LEN))
model.add(Bidirectional(LSTM(config.N_LSTM_CELLS, return_sequences=True, recurrent_dropout=config.RECURRENT_DROPOUT)))
model.add(TimeDistributed(Dense(config.N_OUPUT, activation=config.OUTPUT_ACTIVATION)))
model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER, metrics=config.METRICS)

model.summary()
plot_model(model, show_shapes=True)

history = model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), callbacks=[es, rlp], epochs=config.MAX_EPOCHS)

import matplotlib.pyplot as plt

# Access training history
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Create a figure and specify the subplot layout
plt.figure(figsize=(12, 5))

# Plot training and validation loss
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(train_accuracy, label='Train Accuracy')
plt.plot(val_accuracy, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate model on test data
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

# Predict on test data
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=-1)

# Generate classification report
print("\nClassification Report for Bi-LSTM Model:\n")
print(classification_report(np.argmax(y_test, axis=-1).flatten(), y_pred.flatten(), target_names=tags))

# Print ground truth tags and predicted tags
for i in range(len(X_test)):
    print("Sentence:", ' '.join([words[idx] for idx in X_test[i] if words[idx] != 'ENDPAD']))
    print("Ground Truth Tags:", [tags[tag_idx] for tag_idx in np.argmax(y_test[i], axis=-1) if tags[tag_idx] != ''])
    print("Predicted Tags:", [tags[tag_idx] for tag_idx in y_pred[i] if tags[tag_idx] != ''])
    print()

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt



# Compute confusion matrix
cm = confusion_matrix(np.argmax(y_test, axis=-1).flatten(), y_pred.flatten())

# # Compute normalized confusion matrix
# norm_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]


sorted_tags = [tag for _, tag in sorted(zip(tags, tags))]

# Reorder the confusion matrix based on sorted tags
sorted_cm = np.zeros_like(cm)
for i, tag in enumerate(sorted_tags):
    for j, tag_j in enumerate(sorted_tags):
        sorted_cm[i, j] = cm[tags.index(tag), tags.index(tag_j)]

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(sorted_cm, annot=True, fmt='d', cmap='Reds', xticklabels=sorted_tags, yticklabels=sorted_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('Actual Labels')
plt.title('Confusion Matrix')
plt.show()

# Normalize confusion matrix
conf_matrix_norm = sorted_cm / sorted_cm.sum(axis=1, keepdims=True)

# Plot normalized confusion matrix with different colormap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_norm, annot=True, cmap='Reds', xticklabels=sorted_tags, yticklabels=sorted_tags)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Normalized Confusion Matrix')
plt.show()

# Generate classification report
classification_rep = classification_report(np.argmax(y_test, axis=-1).flatten(), y_pred.flatten(), target_names=tags, output_dict=True)

# Extract precision, recall, and F1-score for each tag
precision = [classification_rep[tag]['precision'] for tag in tags]
recall = [classification_rep[tag]['recall'] for tag in tags]
f1_score = [classification_rep[tag]['f1-score'] for tag in tags]

# Sort tags and corresponding metrics
# sorted_tags = [tag for _, tag in sorted(zip(tags, tags))]
sorted_precision = [prec for _, prec in sorted(zip(tags, precision))]
sorted_recall = [rec for _, rec in sorted(zip(tags, recall))]
sorted_f1_score = [f1 for _, f1 in sorted(zip(tags, f1_score))]

# Create a bar plot
plt.figure(figsize=(12, 6))
bar_width = 0.3
index = range(len(tags))

plt.bar(index, sorted_precision, color='b', width=bar_width, label='Precision')
plt.bar([i + bar_width for i in index], sorted_recall, color='g', width=bar_width, label='Recall')
plt.bar([i + 2 * bar_width for i in index], sorted_f1_score, color='r', width=bar_width, label='F1-score')

plt.xlabel('NER Tags')
plt.ylabel('Scores')
plt.title('Class-wise Metrics for Named Entity Recognition using Bi-LSTM')
plt.xticks([i + bar_width for i in index], sorted_tags, rotation=90)
plt.legend()
plt.tight_layout()
plt.show()